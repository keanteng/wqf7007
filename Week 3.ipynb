{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Text Preprocessing 1 - Tutorial</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-processing common steps:\n",
    "\n",
    "1. Text Cleaning: special characters, HTML tags, new lines \n",
    "2. Tokenization: split text into sentences and words.\n",
    "3. Stop Words Removal: remove words of little value like \"the\", \"and\", \"a\", \"an\".\n",
    "4. Stemming: stripping the affixes from words.\n",
    "5. Lemmatization: converting words to their base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_sentence = \"\"\"A fair number of brave souls who upgraded their SI clock oscillator have\n",
    "shared their experiences for this poll. Please send a brief message detailing\n",
    "your experiences with the procedure. <br> Top speed attained, CPU rated speed,\n",
    "add on cards & adapters, heat sinks, hour of usage per day, floppy disk\n",
    "functionality with 800 and 1.4m floppies are especially requested.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)  # Remove HTML tags\n",
    "    text = re.sub(r\"\\\\n\", \" \", text)  # Remove explicit new-line characters\n",
    "    text = re.sub(r\"[^\\w\\s.]\", \" \", text)  # Remove special characters except for decimal points\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll. please send a brief message detailing your experiences with the procedure. top speed attained cpu rated speed add on cards adapters heat sinks hour of usage per day floppy disk functionality with 800 and 1.4m floppies are especially requested.\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = clean_text(example_sentence)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Khor Kean\n",
      "[nltk_data]     Teng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') #model for sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences:  3\n",
      "['a fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll.', 'please send a brief message detailing your experiences with the procedure.', 'top speed attained cpu rated speed add on cards adapters heat sinks hour of usage per day floppy disk functionality with 800 and 1.4m floppies are especially requested.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sent = sent_tokenize(cleaned_text)\n",
    "print('number of sentences: ', len(tokenized_sent))\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1.4m', 'floppies', 'are', 'especially', 'requested', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(cleaned_text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dont', 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'üòÉ', 'üëç', '#crypto']\n",
      "['Dont', 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'üòÉüëç', '#', 'crypto']\n"
     ]
    }
   ],
   "source": [
    "#Tweet Tokenizer compared to word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = \"Dont take cryptocurrency advice from people on Twitter üòÉüëç #crypto\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenized_tweet = tokenizer.tokenize(tweet)\n",
    "print(tokenized_tweet)\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember That:\n",
    "* Porter Stemmer removes suffixes in a rule-based manner\n",
    "* It does not always return valid English words\n",
    "* Some words retain meaningful roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argu\n",
      "argu\n",
      "argu\n",
      "argu\n"
     ]
    }
   ],
   "source": [
    "# Standard cases\n",
    "print(porter.stem('argue'))\n",
    "print(porter.stem('argued'))\n",
    "print(porter.stem('argues'))\n",
    "print(porter.stem('arguing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "fli\n",
      "fli\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "# Plurals and derivational forms\n",
    "print(porter.stem('running'))    \n",
    "print(porter.stem('runner'))     \n",
    "print(porter.stem('flies'))      \n",
    "print(porter.stem('fly'))\n",
    "print(porter.stem('crying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "univers\n",
      "nation\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "# Complex endings\n",
    "print(porter.stem('happiness'))  \n",
    "print(porter.stem('university'))\n",
    "print(porter.stem('national'))\n",
    "print(porter.stem('generalization'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use PorterStemmer?\n",
    "* For lightweight and rule-based stemming such as text classification and IR systems.\n",
    "* For more linguistically accurate results, consider lemmatization instead (e.g., WordNetLemmatizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Khor Kean\n",
      "[nltk_data]     Teng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argue\n",
      "argue\n",
      "argue\n",
      "arguing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"argue\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"argued\", 'v')) \n",
    "print(lemmatizer.lemmatize(\"argues\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"arguing\", 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "run\n",
      "running\n",
      "fly\n",
      "fly\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", 'a'))\n",
    "print(lemmatizer.lemmatize(\"running\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"running\", 'n'))\n",
    "print(lemmatizer.lemmatize(\"flies\", 'n'))\n",
    "print(lemmatizer.lemmatize(\"flies\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"mice\", 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n"
     ]
    }
   ],
   "source": [
    "#WordNetLemmatizer requires the correct POS tag to be accurate, default is noun\n",
    "print(lemmatizer.lemmatize(\"went\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md') #load the core English language model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SpaCy has Built-in POS tagging (no need to separately tag words like in NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After --> after\n",
      "the --> the\n",
      "cats --> cat\n",
      "fell --> fall\n",
      "asleep --> asleep\n",
      ", --> ,\n",
      "the --> the\n",
      "mice --> mouse\n",
      "went --> go\n",
      "out --> out\n",
      "to --> to\n",
      "play --> play\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('After the cats fell asleep, the mice went out to play.')\n",
    "for token in doc:\n",
    "    print(token,'-->',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original        Lemmatized     \n",
      "==============================\n",
      "a               a              \n",
      "fair            fair           \n",
      "number          number         \n",
      "of              of             \n",
      "brave           brave          \n",
      "souls           soul           \n",
      "who             who            \n",
      "upgraded        upgrade        \n",
      "their           their          \n",
      "si              si             \n",
      "clock           clock          \n",
      "oscillator      oscillator     \n",
      "have            have           \n",
      "shared          share          \n",
      "their           their          \n",
      "experiences     experience     \n",
      "for             for            \n",
      "this            this           \n",
      "poll            poll           \n",
      ".               .              \n",
      "please          please         \n",
      "send            send           \n",
      "a               a              \n",
      "brief           brief          \n",
      "message         message        \n",
      "detailing       detail         \n",
      "your            your           \n",
      "experiences     experience     \n",
      "with            with           \n",
      "the             the            \n",
      "procedure       procedure      \n",
      ".               .              \n",
      "top             top            \n",
      "speed           speed          \n",
      "attained        attain         \n",
      "cpu             cpu            \n",
      "rated           rate           \n",
      "speed           speed          \n",
      "add             add            \n",
      "on              on             \n",
      "cards           card           \n",
      "adapters        adapter        \n",
      "heat            heat           \n",
      "sinks           sink           \n",
      "hour            hour           \n",
      "of              of             \n",
      "usage           usage          \n",
      "per             per            \n",
      "day             day            \n",
      "floppy          floppy         \n",
      "disk            disk           \n",
      "functionality   functionality  \n",
      "with            with           \n",
      "800             800            \n",
      "and             and            \n",
      "1.4             1.4            \n",
      "m               m              \n",
      "floppies        floppy         \n",
      "are             be             \n",
      "especially      especially     \n",
      "requested       request        \n",
      ".               .              \n"
     ]
    }
   ],
   "source": [
    "#lemmatize our original example sentence\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Extract original words and their lemmatized forms\n",
    "original = [token.text for token in doc]\n",
    "lemmatized = [token.lemma_.lower() for token in doc]\n",
    "\n",
    "# Display results in aligned format\n",
    "print(f\"{'Original':<15} {'Lemmatized':<15}\")\n",
    "print(\"=\" * 30)\n",
    "for orig, lem in zip(original, lemmatized):\n",
    "    print(f\"{orig:<15} {lem:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Khor Kean\n",
      "[nltk_data]     Teng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique stop words: 198\n",
      "Sample stop words: ['until', 'being', 'as', \"it'd\", \"shan't\", 'can', 'both', 'his', 'isn', 'its', 'such', 't', 'now', 'some', 'the', 'then', 'won', 'very', 'other', 'should', 'doing', \"should've\", \"he'd\", 'hers', 'i', 'their', \"they'd\", 'couldn', 'any', 'so', \"hasn't\", 'above', 'doesn', 'out', 'it', 'do', 'at', 'ma', \"haven't\", 'our', 'weren', 'few', 'about', \"she's\", 'why', 'there', 'yours', 'to', \"weren't\", \"don't\", 'on', 'don', 'where', 'ain', 'didn', 'your', 'with', 'further', 've', \"he's\", 'a', 'all', \"isn't\", 'just', 'an', 'himself', 'these', \"he'll\", 'before', 'does', 'wouldn', 'll', 'theirs', 'them', 'wasn', 'off', 'myself', 'y', \"wasn't\", \"you've\", \"mightn't\", 'having', 'ours', 're', 'when', \"doesn't\", 'of', 'how', \"that'll\", 'me', 'aren', 'who', 'him', 'nor', 'against', 'too', 'itself', 'because', 'or', 'which']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get stop words list\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Convert to a set to remove duplicates\n",
    "unique_stopwords = set(stop)\n",
    "\n",
    "# Convert back to a list\n",
    "stop = list(unique_stopwords)\n",
    "\n",
    "print(\"Total unique stop words:\", len(stop))\n",
    "print(\"Sample stop words:\", stop[:100])  # Print the first 100 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique stop words: 326\n",
      "Sample stop words: ['say', 'now', 'very', 'other', 'should', 'last', 'formerly', 'nine', 'at', 'seem', 'others', 'few', 'third', 'why', 'there', 'many', 'became', 'on', 'eight', '‚Äôm', '‚Äôd', 'further', \"'d\", 'himself', 'wherein', 'becoming', 'four', 'every', 'mostly', 'off', 'would', 'somewhere', 'one', 'how', 'around', 'be', 'am', 'had', 'between', '‚Äòd', 'moreover', 'amongst', 'into', 'serious', 'whether', 'else', 'move', 'empty', 'whenever', 'nevertheless', 'were', 'eleven', 'n‚Äòt', 'down', 'put', 'while', 'though', 'through', 'have', 'has', 'back', 'besides', 'she', 'but', 'will', 'using', 'no', 'really', 'not', 'by', 'seems', 'otherwise', 'together', 'neither', 'either', 'along', 'can', 'also', 'both', 'his', 'its', 'then', 'done', 'per', 'top', 'might', 'indeed', 'something', 'becomes', 'former', 'n‚Äôt', 'yet', 'towards', 'please', 'upon', 'thence', 'ca', 'seemed', 'with', 'whence']\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words=list(STOP_WORDS)\n",
    "\n",
    "print(\"Total unique stop words:\", len(stop_words))\n",
    "print(\"Sample stop words:\", stop_words[:100])  # First 100 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_removed = [word for word in lemmatized if word not in stop_words]\n",
    "removed_arr = [word for word in lemmatized if word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fair', 'number', 'brave', 'soul', 'upgrade', 'si', 'clock', 'oscillator', 'share', 'experience', 'poll', '.', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', '.', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', '800', '1.4', 'm', 'floppy', 'especially', 'request', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'of', 'who', 'their', 'have', 'their', 'for', 'this', 'please', 'a', 'your', 'with', 'the', 'top', 'on', 'of', 'per', 'with', 'and', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(removed_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
